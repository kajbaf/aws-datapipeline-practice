{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9f617b-e1bd-4dbc-bc4b-91b6430c3907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/prj/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6d02036b-267d-418a-bcf6-555382222e10;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 309ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6d02036b-267d-418a-bcf6-555382222e10\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "25/07/18 17:05:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 1. SparkSession with Delta Config\n",
    "# ------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Test DeltaLake Connection\")\n",
    "    .master(\"spark://spark:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb380163-27a6-44b9-ad8d-42af8d1cb65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---+--------------+\n",
      "|customer_id|gender|age|payment_method|\n",
      "+-----------+------+---+--------------+\n",
      "|    C241288|Female| 28|   Credit Card|\n",
      "|    C111565|  Male| 21|    Debit Card|\n",
      "|    C266599|  Male| 20|          Cash|\n",
      "+-----------+------+---+--------------+\n",
      "only showing top 3 rows\n",
      "Sales Data:\n",
      "+----------+-----------+--------+--------+-------+------------+--------------+\n",
      "|invoice_no|customer_id|category|quantity|  price|invoice_date| shopping_mall|\n",
      "+----------+-----------+--------+--------+-------+------------+--------------+\n",
      "|   I138884|    C241288|Clothing|       5| 1500.4|  05-08-2022|        Kanyon|\n",
      "|   I317333|    C111565|   Shoes|       3|1800.51|  12-12-2021|Forum Istanbul|\n",
      "|   I127801|    C266599|Clothing|       1| 300.08|  09-11-2021|     Metrocity|\n",
      "+----------+-----------+--------+--------+-------+------------+--------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 2. Load Source Data\n",
    "# ------------------------------\n",
    "customer_df = spark.read.parquet(\"/s3-datalake/source/customer_data.parquet\")\n",
    "sales_df = spark.read.csv(\"/s3-datalake/source/sales_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Customer Data:\")\n",
    "customer_df.show(3)\n",
    "\n",
    "print(\"Sales Data:\")\n",
    "sales_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5629c9-eb5b-44ce-a429-67f0dc41ae17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta files written to: /s3-datalake/backup/sales_delta\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 3. Write Sales Data as a Delta Table\n",
    "# ------------------------------\n",
    "output_path = \"/s3-datalake/backup/sales_delta\"\n",
    "\n",
    "sales_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(\"Delta files written to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "424a6cc5-fae9-4dc8-b0ed-304b5dedb66e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/18 17:06:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/18 17:06:40 WARN UpdateCommand: Could not validate number of records due to missing statistics.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+--------+-----+------------+--------------+\n",
      "|invoice_no|customer_id|       category|quantity|price|invoice_date| shopping_mall|\n",
      "+----------+-----------+---------------+--------+-----+------------+--------------+\n",
      "|      TEST|    C870742|Food & Beverage|       4|20.92|  07-10-2021|  Istinye Park|\n",
      "|      TEST|    C289408|      Cosmetics|       2|81.32|  11-10-2021|        Kanyon|\n",
      "|      TEST|    C138416|Food & Beverage|       3|15.69|  14-01-2022|Viaport Outlet|\n",
      "+----------+-----------+---------------+--------+-----+------------+--------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 4. Run SQL on the Delta Table\n",
    "# ------------------------------\n",
    "## Run an Update operation\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE delta.`{output_path}` \n",
    "    SET invoice_no = 'TEST'\n",
    "\"\"\")\n",
    "\n",
    "# Read and display\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM delta.`{output_path}`\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44c634f-9786-4d55-b37b-5de5ed88025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean-up complete.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 5. Cleanup Delta Table and Files\n",
    "# ------------------------------\n",
    "# Drop metadata reference\n",
    "spark.sql(f\"\"\"\n",
    "    DROP TABLE IF EXISTS delta.`{output_path}`\n",
    "\"\"\")\n",
    "\n",
    "# Remove files\n",
    "!rm -Rf {output_path}\n",
    "\n",
    "print(\"Clean-up complete.\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
