{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9f617b-e1bd-4dbc-bc4b-91b6430c3907",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Setup Data Loading Job\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514c40f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import hashlib\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, DataFrame\n",
    "from datetime import datetime, UTC\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Define Job Parameters\n",
    "# ----------------------------------------\n",
    "DAG_ID = \"Load CRM Data\"\n",
    "JOB_DATE = f\"{datetime.now(UTC):%Y-%m-%d}\"\n",
    "JOB_NAME = f\"{DAG_ID} {JOB_DATE}\"\n",
    "DELTA_LAKE = \"/s3-datalake/lakehouse\"\n",
    "\n",
    "source_customer_data = \"/s3-datalake/source/customer_data.parquet\"\n",
    "source_sale_data = \"/s3-datalake/source/sales_data.csv\"\n",
    "\n",
    "# Setup Logging\n",
    "# ----------------------------------------\n",
    "log.basicConfig(level=log.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bf4481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/prj/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f1e2ae13-0417-46db-8a44-e6652a3914a5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 167ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f1e2ae13-0417-46db-8a44-e6652a3914a5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/07/21 01:40:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-07-21 01:40:17,349 - INFO - Started session `Load CRM Data 2025-07-21`\n"
     ]
    }
   ],
   "source": [
    "# 2. Create SparkSession with Delta Config\n",
    "# ----------------------------------------\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(JOB_NAME)\n",
    "    .master(\"spark://spark:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.warehouse.dir\", DELTA_LAKE)\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "log.info(f\"Started session `{JOB_NAME}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa6b4f2-c902-4221-9c5c-43b5bed764ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setup initial SCHEMAs if required\n",
    "spark.sql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS bronze LOCATION '{DELTA_LAKE}/bronze'\"\n",
    ").collect()\n",
    "spark.sql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS silver LOCATION '{DELTA_LAKE}/silver'\"\n",
    ").collect()\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS gold LOCATION '{DELTA_LAKE}/gold'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f597943",
   "metadata": {},
   "source": [
    "# Bronze Layer\n",
    "---\n",
    "This layer serves as the **raw ingestion zone**, where data is landed from external systems into our Lakehouse with **minimal transformation**. It preserves the original fidelity of the data, making it ideal for auditability and replay.\n",
    "\n",
    "My typical practice in the Bronze layer includes the following steps:\n",
    "\n",
    "1. **Ingest data from external sources**, such as:\n",
    "   - Application databases (full or incremental extracts)\n",
    "   - Change Data Capture (CDC) systems\n",
    "   - Messaging systems (e.g., Kafka, Kinesis, Pub/Sub)\n",
    "   - External APIs, SFTP, blob/cloud storage, etc.\n",
    "\n",
    "2. **Apply incremental loading logic** where applicable  \n",
    "   - For example, using `kafka_timestamp` or `offset` from Kafka topics\n",
    "\n",
    "3. **Standardize column names** to align with internal naming conventions used in the Lakehouse or Warehouse\n",
    "\n",
    "4. **Add system-level metadata** for observability and debugging:\n",
    "   - Fields like `_system`, `_source`, `_ingestion_date`, etc.\n",
    "   - These are useful for tracking data lineage and pipeline behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f042cee-8ff7-49da-b561-1e4c952af89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 01:40:20,585 - INFO - Creating managed table bronze.customer in schema bronze\n",
      "2025-07-21 01:40:27,332 - INFO - Creating managed table bronze.sale in schema bronze\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "def _add_metadata(df: DataFrame, meta: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add static metadata columns to a Spark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input Spark DataFrame.\n",
    "        meta (dict): A dictionary of column names and their corresponding literal values.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with added metadata columns.\n",
    "    \"\"\"\n",
    "    meta_columns = {col: F.lit(val) for col, val in meta.items()}\n",
    "    df_with_meta = df.withColumns(meta_columns).withColumn(\n",
    "        \"_ingestion_timestamp\", F.current_timestamp()\n",
    "    )\n",
    "\n",
    "    return df_with_meta\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    data: DataFrame, table: str, metadata: dict, schema: str = \"bronze\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a DataFrame into a Delta Lake bronze table, partitioned by _ingestion_date.\n",
    "    Creates the table if it does not exist; otherwise, overwrites the partition for the current ingestion date.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): The source Spark DataFrame to load.\n",
    "        table (str): The target table name (used in the path and metastore).\n",
    "        metadata (dict): Static metadata to add to the DataFrame before loading.\n",
    "    \"\"\"\n",
    "    full_table_name = f\"{schema}.{table.lower()}\"\n",
    "\n",
    "    # Add metadata\n",
    "    data_with_metadata = _add_metadata(data, metadata)\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    if not spark.catalog.tableExists(full_table_name):\n",
    "        log.info(f\"Creating managed table {full_table_name} in schema {schema}\")\n",
    "\n",
    "        data_with_metadata.write.format(\"delta\").partitionBy(\"_job_date\").mode(\n",
    "            \"overwrite\"\n",
    "        ).saveAsTable(full_table_name)\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS bronze.{table}\n",
    "            USING DELTA\n",
    "        \"\"\")\n",
    "    else:\n",
    "        log.info(\n",
    "            f\"Overwriting existing Delta partition for {JOB_DATE} in table {full_table_name}\"\n",
    "        )\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            DELETE FROM {full_table_name}\n",
    "            WHERE _job_date = '{JOB_DATE}'\n",
    "        \"\"\")\n",
    "\n",
    "        data_with_metadata.write.format(\"delta\").mode(\"append\").partitionBy(\n",
    "            \"_job_date\"\n",
    "        ).saveAsTable(full_table_name)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Job: Load customer_data to bronze layer\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Metadata for ingestion\n",
    "metadata_customer = {\n",
    "    \"_system\": \"CRM\",\n",
    "    \"_source\": source_customer_data,\n",
    "    \"_type\": \"Parquet\",\n",
    "    \"_job_date\": JOB_DATE,\n",
    "}\n",
    "\n",
    "# Read source data\n",
    "customer_df = spark.read.parquet(source_customer_data)\n",
    "\n",
    "# Load to Delta Lake\n",
    "load_data(data=customer_df, table=\"customer\", metadata=metadata_customer)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Job: Load sale_data to bronze layer\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Metadata for ingestion\n",
    "metadata_sale = {\n",
    "    \"_system\": \"CRM\",\n",
    "    \"_source\": source_sale_data,\n",
    "    \"_type\": \"CSV\",\n",
    "    \"_job_date\": JOB_DATE,\n",
    "}\n",
    "\n",
    "# Read source data with header, and assuming all STRING data\n",
    "sale_df = (\n",
    "    spark.read.option(\"header\", True).option(\"inferSchema\", False).csv(source_sale_data)\n",
    ")\n",
    "\n",
    "# Load to Delta Lake\n",
    "load_data(data=sale_df, table=\"sale\", metadata=metadata_sale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef179c-dff9-4df8-87ff-aa19eced9242",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Silver Layer\n",
    "---\n",
    "In this layer, we transform raw Bronze data into **cleaned, structured, and enriched datasets**, ready for modeling and analytics.\n",
    "\n",
    "My typical actions in the Silver layer include:\n",
    "\n",
    "1. Load data incrementally from the Bronze layer using `_job_date`\n",
    "   - This approach simplifies handling late-arriving data, as late `event_timestamps` are still processed\n",
    "   - More complex watermark or merge strategies can be added if needed\n",
    "\n",
    "2. Clean and deduplicate records based on business rules\n",
    "\n",
    "3. Cast fields to expected data types (e.g., `timestamp`, `int`, `boolean`) per Lakehouse standards\n",
    "\n",
    "4. Generate surrogate keys for primary entities where applicable\n",
    "\n",
    "5. Add or enrich metadata columns (e.g., `_processed_at`, flags)\n",
    "\n",
    "6. Validate schema and content to ensure Silver layer standards are met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8caae447-cac2-4c13-acc8-747a0646b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_silver_customer = \"\"\"\n",
    "        SELECT\n",
    "            * EXCEPT(gender, payment_method, age),\n",
    "            COALESCE(payment_method, 'unknown') AS payment_method,\n",
    "            CASE\n",
    "                WHEN LOWER(gender) IN ('male', 'female')\n",
    "                    THEN lower(gender)\n",
    "                ELSE 'unknown'\n",
    "            END AS gender,\n",
    "            TRY_TO_NUMBER(age, \"999\") AS age\n",
    "        FROM bronze.customer c\n",
    "        WHERE customer_id IS NOT NULL\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "invoice_date_format = \"dd-MM-yyyy\"\n",
    "sql_silver_sale = f\"\"\"\n",
    "        WITH valid_sales AS (\n",
    "            SELECT *,\n",
    "                RANK() OVER (\n",
    "                    PARTITION BY invoice_no, customer_id\n",
    "                    ORDER BY TO_DATE(invoice_date, '{invoice_date_format}')\n",
    "                ) AS rank\n",
    "            FROM bronze.sale\n",
    "            WHERE invoice_no IS NOT NULL\n",
    "              AND customer_id IS NOT NULL\n",
    "        )\n",
    "        SELECT * EXCEPT(invoice_date, shopping_mall, price, quantity),\n",
    "            TO_DATE(invoice_date, '{invoice_date_format}') AS invoice_date,\n",
    "            LOWER(shopping_mall) AS shopping_mall,\n",
    "            CAST(price AS DOUBLE) AS price,\n",
    "            CAST(quantity AS INT) AS quantity\n",
    "        FROM valid_sales\n",
    "        WHERE rank = 1\n",
    "          AND CAST(price AS DOUBLE) > 0\n",
    "          AND CAST(quantity AS INT) > 0\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d44c634f-9786-4d55-b37b-5de5ed88025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 01:40:31,126 - INFO - Populating silver.customer from bronze.customer\n",
      "25/07/21 01:40:31 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2025-07-21 01:40:39,653 - INFO - Populating silver.sale from bronze.sale        \n",
      "2025-07-21 01:40:39,772 - INFO - Destination table doesn't exist yet, running initial increment\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Config Section\n",
    "# -------------------------------\n",
    "DEFAULT_LOOKBACK_DAYS = 1\n",
    "\n",
    "silver_configs = {\n",
    "    \"customer\": {\n",
    "        \"bronze_table\": \"bronze.customer\",\n",
    "        \"load_type\": \"full\",  # can also be \"scd\"\n",
    "        \"sql\": sql_silver_customer,\n",
    "        \"surrogate_key_columns\": [\"customer_id\"],\n",
    "    },\n",
    "    \"sale\": {\n",
    "        \"bronze_table\": \"bronze.sale\",\n",
    "        \"load_type\": \"incremental\",\n",
    "        \"incremental_column\": \"invoice_date\",\n",
    "        \"loopback\": 1,\n",
    "        \"sql\": sql_silver_sale,\n",
    "        \"surrogate_key_columns\": [\"invoice_no\", \"customer_id\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "def _add_surrogate_key(\n",
    "    df: DataFrame, columns: list, output_col: str = \"surrogate_key\"\n",
    ") -> DataFrame:\n",
    "    concat_expr = F.concat_ws(\"||\", *[F.col(col).cast(\"string\") for col in columns])\n",
    "    return df.withColumn(output_col, F.sha1(concat_expr))\n",
    "\n",
    "\n",
    "def _add_silver_metadata(df: DataFrame, key_columns: list) -> DataFrame:\n",
    "    return (\n",
    "        _add_surrogate_key(df, key_columns)\n",
    "        .withColumn(\"_updated_at\", F.current_timestamp())\n",
    "        .withColumn(\"_created_at\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_into_silver(\n",
    "    new_data: DataFrame,\n",
    "    table_name: str,\n",
    "    key_column: str = \"surrogate_key\",\n",
    "    partition_column: str = None,\n",
    ") -> None:\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        new_data.createOrReplaceTempView(\"incoming\")\n",
    "\n",
    "        set_expr = \",\\n\".join(\n",
    "            [\n",
    "                f\"{col} = s.{col}\"\n",
    "                for col in new_data.columns\n",
    "                if col not in [\"_created_at\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO {table_name} AS t\n",
    "            USING incoming AS s\n",
    "            ON t.{key_column} = s.{key_column}\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET {set_expr}\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT *\n",
    "        \"\"\")\n",
    "    else:\n",
    "        writer = new_data.write.format(\"delta\").mode(\"overwrite\")\n",
    "        if partition_column and partition_column in new_data.columns:\n",
    "            writer = writer.partitionBy(partition_column)\n",
    "        writer.saveAsTable(table_name)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main Processing Function\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "def process_table(table_name: str, config: dict) -> None:\n",
    "    bronze_table = config[\"bronze_table\"]\n",
    "    sql_logic = config[\"sql\"]\n",
    "    sk_cols = config[\"surrogate_key_columns\"]\n",
    "    load_type = config.get(\"load_type\", \"full\")\n",
    "    incr_col = config.get(\"incremental_column\")\n",
    "    loopback = config.get(\"loopback\", DEFAULT_LOOKBACK_DAYS)\n",
    "\n",
    "    full_table_name = f\"silver.{table_name}\"\n",
    "\n",
    "    # Custom transformation logic\n",
    "    log.info(f\"Populating {full_table_name} from {bronze_table}\")\n",
    "    transformed_df = spark.sql(sql_logic)\n",
    "\n",
    "    table_exists = spark.catalog.tableExists(full_table_name)\n",
    "\n",
    "    # Only apply incremental filter if the Silver table exists\n",
    "    if load_type == \"incremental\" and incr_col and table_exists:\n",
    "        transformed_df = transformed_df.filter(\n",
    "            F.col(incr_col) >= F.date_sub(F.current_date(), loopback)\n",
    "        )\n",
    "        log.info(\n",
    "            f\"Applying incremental filter: {incr_col} >= current_date - {loopback} days\"\n",
    "        )\n",
    "    elif load_type == \"incremental\":\n",
    "        log.info(\"Destination table doesn't exist yet, running initial increment\")\n",
    "\n",
    "    # Add metadata\n",
    "    tagged_df = _add_silver_metadata(transformed_df, sk_cols)\n",
    "\n",
    "    # Merge\n",
    "    partition_col = incr_col if load_type == \"incremental\" else None\n",
    "    merge_into_silver(\n",
    "        tagged_df,\n",
    "        full_table_name,\n",
    "        key_column=\"surrogate_key\",\n",
    "        partition_column=partition_col,\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Run All Tables\n",
    "# -------------------------------\n",
    "\n",
    "for table_name, config in silver_configs.items():\n",
    "    process_table(table_name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c343cc-03f4-47d4-8f50-9a01537b1e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|   silver| customer|      false|\n",
      "|   silver|     sale|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+-------+--------------------+-----+----------+--------------------+----+------------+----------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|invoice_no|customer_id|       category|_system|             _source|_type| _job_date|_ingestion_timestamp|rank|invoice_date|   shopping_mall|  price|quantity|       surrogate_key|         _updated_at|         _created_at|\n",
      "+----------+-----------+---------------+-------+--------------------+-----+----------+--------------------+----+------------+----------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|   I136504|    C758807|       Souvenir|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|mall of istanbul|  58.65|       5|78729719a0275ddf0...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I146688|    C266266|           Toys|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|mall of istanbul|  35.84|       1|83fffe5f688c84eba...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I149808|    C268986|Food & Beverage|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|    istinye park|  15.69|       3|fc967306cc6ee6f84...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I153040|    C849671|Food & Beverage|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|     cevahir avm|  15.69|       3|8e16b01a5cc52f554...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I163278|    C112291|       Clothing|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|  viaport outlet| 300.08|       1|ff60602c1f8b67002...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I174918|    C281747|       Clothing|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|       metrocity|1200.32|       4|b9e0bf49ba6405da7...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I177368|    C137210|       Souvenir|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|          kanyon|  46.92|       4|ff686ab621f465f92...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I177616|    C124628|     Technology|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|       metrocity| 4200.0|       4|db0a48ea7cc6d9f0e...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I185878|    C316232|       Clothing|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|          kanyon| 600.16|       2|c30df8d12f1305ba4...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I192741|    C294695|       Clothing|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|          kanyon|1200.32|       4|f4e76a74dbe1f397e...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I193659|    C234296|       Clothing|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|mall of istanbul| 900.24|       3|35542e7bf9b904ef4...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I194249|    C258668|           Toys|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|          kanyon|  35.84|       1|b1ae8ef236eb65a49...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I195976|    C131547|Food & Beverage|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|    metropol avm|  10.46|       2|7ec4a0ac3e6e48d3d...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I196326|    C110469|          Books|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|    metropol avm|  75.75|       5|042b461aa1cd738c7...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I201847|    C236448|           Toys|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|mall of istanbul|  71.68|       2|7b29b09346a7bb9ca...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I205386|    C800801|      Cosmetics|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|       metrocity|  203.3|       5|db5ad3619e108f0aa...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I212359|    C207486|          Shoes|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|  forum istanbul|1800.51|       3|6ebb96cd9abd651b7...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I214560|    C191282|Food & Beverage|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|    metropol avm|  10.46|       2|575ea3ed429501664...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I217191|    C894974|       Clothing|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|    istinye park| 600.16|       2|7ec2b1aae24325f0f...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "|   I218737|    C129491|      Cosmetics|    CRM|/s3-datalake/sour...|  CSV|2025-07-21|2025-07-21 01:40:...|   1|  2021-02-28|  forum istanbul|  40.66|       1|795c5baccae355db2...|2025-07-21 01:40:...|2025-07-21 01:40:...|\n",
      "+----------+-----------+---------------+-------+--------------------+-----+----------+--------------------+----+------------+----------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables in silver\").show()\n",
    "spark.sql(\"select * from silver.sale\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2146b8d-20ae-4379-bdec-a3832952eaff",
   "metadata": {},
   "source": [
    "# Gold Layer\n",
    "---\n",
    "\n",
    "In this layer, we create clean, analytics-ready **dimension** and **fact** tables following the **Star Schema** pattern, optimized for consumption by BI tools and data analysts.\n",
    "\n",
    "This is where raw and semi-structured data is transformed into a **semantic, consistent, and high-performance model** that enables:\n",
    "- Self-serve exploration\n",
    "- Performance benchmarking\n",
    "- Time-series trend analysis\n",
    "- Operational and strategic reporting\n",
    "\n",
    "Some usual techniques include:\n",
    "\n",
    "1. Applying slowly-changing dimension logic (type 1.5 or 2) for dimensional data  \n",
    "   - Decision depends on the level of historical info required\n",
    "2. Create FAT Table (or One Big Table) for very large event streams\n",
    "   - An OBT or fat table is a fact table that is already poplulated with data from dimension tables\n",
    "2. Include additional details from the Date dimension for better analysis of cyclical behaviour\n",
    "3. Generate analytical columns like `age_band` or `is_weekend`\n",
    "4. Include surrogate key from dimensions instead of the normal key\n",
    "5. Support Incremental Loading for Large Fact Tables\n",
    "   - Apply incremental filters to avoid full reloads  \n",
    "   - Maintain partitions (e.g., by `invoice_date`) for efficient pruning of data in analytical queries\n",
    "6. De-duplication and Consistency Enforcement\n",
    "   - Ensure data quality by enforcing unique constraints on surrogate keys  \n",
    "   - Join facts only to the current version of each dimension row\n",
    "7. Materialize Common Business Metrics\n",
    "   - Derive fields like `gross_amount`, `avg_basket_size`, `customer_tenure`, etc.  \n",
    "   - This reduces repetitive logic in dashboards or downstream aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad795ff9-4a01-4bc4-82cd-44371f33b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "### gold_layer_queries\n",
    "\n",
    "sql_dim_customer = \"\"\"\n",
    "SELECT\n",
    "    surrogate_key                      AS customer_sk,\n",
    "    customer_id,\n",
    "    gender,\n",
    "    age,\n",
    "    CASE                                -- derive age_band once\n",
    "        WHEN age < 18  THEN 'under 18'\n",
    "        WHEN age BETWEEN 18 AND 24          THEN '18‑24'\n",
    "        WHEN age BETWEEN 25 AND 34          THEN '25‑34'\n",
    "        WHEN age BETWEEN 35 AND 44          THEN '35‑44'\n",
    "        WHEN age BETWEEN 45 AND 54          THEN '45‑54'\n",
    "        WHEN age BETWEEN 55 AND 64          THEN '55‑64'\n",
    "        WHEN age >= 65 THEN '65+'\n",
    "        ELSE 'unknown'\n",
    "    END                                   AS age_band,\n",
    "    payment_method,\n",
    "    current_date()                        AS _created_at,\n",
    "    current_date()                        AS _updated_at,\n",
    "    TRUE                                   AS _is_current\n",
    "FROM silver.customer\n",
    "\"\"\"\n",
    "\n",
    "scd_dim_customer = \"\"\"\n",
    "MERGE INTO gold.dim_customer AS t\n",
    "USING (\n",
    "    SELECT *\n",
    "    FROM {source}\n",
    ") AS s\n",
    "ON  t.{key}  = s.{key} AND t._is_current  = TRUE\n",
    "WHEN MATCHED THEN UPDATE\n",
    "    SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "WHEN NOT MATCHED BY SOURCE THEN\n",
    "    UPDATE SET _is_current = FALSE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c734ca2-eab1-4a75-9d66-630bd0a6a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_fact_sale = \"\"\"\n",
    "SELECT\n",
    "    s.surrogate_key                                 AS sale_sk,\n",
    "    s.invoice_no,\n",
    "    COALESCE(dc.customer_sk, 'unknown')             AS customer_sk,\n",
    "    s.shopping_mall,\n",
    "    s.invoice_date,\n",
    "    s.category,\n",
    "    s.quantity,\n",
    "    s.price                                         AS unit_price,\n",
    "    s.quantity * s.price                            AS gross_amount,\n",
    "    CASE\n",
    "        WHEN price < 50 THEN 'low'\n",
    "        WHEN price BETWEEN 50 AND 199.99 THEN 'medium'\n",
    "        WHEN price BETWEEN 200 AND 999.99 THEN 'high'\n",
    "        WHEN price >= 1000 THEN 'premium'\n",
    "        ELSE 'unknown'\n",
    "    END                                             AS price_bucket,    \n",
    "    dayofweek(s.invoice_date) IN (1,7)    -- Sun=1,Sat=7\n",
    "                            AS is_weekend,\n",
    "    dayofweek(s.invoice_date)                       AS week_day,\n",
    "    dayofmonth(s.invoice_date)                      AS month_day,\n",
    "    date_trunc('week',  s.invoice_date)             AS week_start_date,\n",
    "    date_trunc('month', s.invoice_date)             AS month_start_date,\n",
    "    concat(year(s.invoice_date), '-Q', quarter(s.invoice_date))\n",
    "                                                   AS fiscal_quarter,\n",
    "    current_date()                        AS _created_at,\n",
    "    current_date()                        AS _updated_at\n",
    "FROM silver.sale        s\n",
    "LEFT JOIN gold.dim_customer   dc\n",
    "       ON  dc.customer_id = s.customer_id\n",
    "       AND dc._is_current = TRUE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25c5c9d4-110f-4179-806a-ef3b0e3f2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 01:45:56 WARN MapPartitionsRDD: RDD 158 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n",
      "2025-07-21 01:45:56,913 - INFO - Destination table doesn't exist yet, running initial increment\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "gold_layer_config = {\n",
    "    \"dimensions\": [\n",
    "        {\n",
    "            \"table_name\": \"gold.dim_customer\",\n",
    "            \"sql_query\": sql_dim_customer,\n",
    "            \"scd_query\": scd_dim_customer,\n",
    "            \"key\": \"customer_sk\",\n",
    "        },\n",
    "    ],\n",
    "    \"facts\": [\n",
    "        {\n",
    "            \"table_name\": \"gold.fact_sale\",\n",
    "            \"sql_query\": sql_fact_sale,\n",
    "            \"partition_column\": \"invoice_date\",\n",
    "            \"incremental_column\": \"invoice_date\",\n",
    "            \"lookback_days\": 1,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def load_dim_table(table_name: str, sql_query: str, scd_query: str, key: str) -> None:\n",
    "    \"\"\"\n",
    "    Loads a dimension table, optionally apply SCD logic if it exists, otherwise creates it.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the dimension table in metastore.\n",
    "        sql_query (str): SQL to select the latest dim version from Silver.\n",
    "        scd_query (str): SQL to apply type 1.5 or 2 SCD logic.\n",
    "        key (str): Join key used in the MERGE condition.\n",
    "    \"\"\"\n",
    "    staged_df = spark.sql(sql_query)\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        stage_name = table_name.split(\".\")[-1] + \"_staged\"\n",
    "        staged_df.createOrReplaceTempView(stage_name)\n",
    "        spark.sql(scd_query.format(source=stage_name, key=key))\n",
    "    else:\n",
    "        staged_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "\n",
    "def load_fact_table(\n",
    "    table_name: str,\n",
    "    sql_query: str,\n",
    "    partition_column: str = None,\n",
    "    incremental_column: str = None,\n",
    "    lookback_days: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Loads a fact table. Supports incremental load and optional partitioning.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the fact table in metastore.\n",
    "        sql_query (str): SQL to select fact data from Silver layer.\n",
    "        partition_column (str, optional): Column to partition the fact table by.\n",
    "        incremental_column (str, optional): Column to filter for incremental load.\n",
    "        lookback_days (int): How far back to look for late data.\n",
    "    \"\"\"\n",
    "    table_exists = spark.catalog.tableExists(table_name)\n",
    "    df = spark.sql(sql_query)\n",
    "\n",
    "    if incremental_column and incremental_column in df.columns and table_exists:\n",
    "        log.info(\n",
    "            f\"Applying incremental filter: {incremental_column} >= current_date - {lookback_days} days\"\n",
    "        )\n",
    "        df = df.filter(\n",
    "            F.col(incremental_column) >= F.date_sub(F.current_date(), lookback_days)\n",
    "        )\n",
    "    elif incremental_column and not table_exists:\n",
    "        log.info(\"Destination table doesn't exist yet, running initial increment\")\n",
    "\n",
    "    writer = df.write.format(\"delta\").mode(\"append\")\n",
    "\n",
    "    if partition_column and partition_column in df.columns:\n",
    "        writer = writer.partitionBy(partition_column)\n",
    "\n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "\n",
    "# Load dimensions\n",
    "for dim in gold_layer_config[\"dimensions\"]:\n",
    "    load_dim_table(\n",
    "        table_name=dim[\"table_name\"],\n",
    "        sql_query=dim[\"sql_query\"],\n",
    "        scd_query=dim[\"scd_query\"],\n",
    "        key=dim[\"key\"],\n",
    "    )\n",
    "\n",
    "# Load facts\n",
    "for fact in gold_layer_config[\"facts\"]:\n",
    "    load_fact_table(\n",
    "        table_name=fact[\"table_name\"],\n",
    "        sql_query=fact[\"sql_query\"],\n",
    "        partition_column=fact.get(\"partition_column\"),\n",
    "        incremental_column=fact.get(\"incremental_column\"),\n",
    "        lookback_days=fact.get(\"lookback_days\", 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eae8d629-b11f-4ff6-88eb-b6d28bb0ecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|          tableName|isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|   bronze|           customer|      false|\n",
      "|   bronze|               sale|      false|\n",
      "|         |dim_customer_staged|       true|\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "+---------+-------------------+-----------+\n",
      "|namespace|          tableName|isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|   silver|           customer|      false|\n",
      "|   silver|               sale|      false|\n",
      "|         |dim_customer_staged|       true|\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "+---------+-------------------+-----------+\n",
      "|namespace|          tableName|isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|     gold|       dim_customer|      false|\n",
      "|     gold|          fact_sale|      false|\n",
      "|         |dim_customer_staged|       true|\n",
      "+---------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN bronze\").show()\n",
    "spark.sql(\"SHOW TABLES IN silver\").show()\n",
    "spark.sql(\"SHOW TABLES IN gold\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
